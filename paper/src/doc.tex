\documentclass{amsart}

% packages
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{mathabx}

% bibliography
\usepackage[backend=biber]{biblatex}
\DeclareFieldFormat{postnote}{#1}
\addbibresource{bibliography.bib}

% page layout
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=3.5cm,bmargin=2.5cm,lmargin=2.6cm,rmargin=2.6cm}


% theorem styles
\theoremstyle{plain}
\newtheorem{proposition}[subsection]{Proposition}
\newtheorem{corollary}[subsection]{Corollary}
\newtheorem{lemma}[subsection]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[subsection]{Definition}
\newtheorem{remark}[subsection]{Remark}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\Pot}{Pot}
\DeclareMathOperator{\Nodes}{Nodes}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\rootnode}{root}
\DeclareMathOperator{\height}{height}

\begin{document}

\title{Punctuation Reconstruction using Word Embeddings}
\author{Martin Bidlingmaier}
\date{17.02.2016}
\maketitle

\section{Abstract}
  Some Abstract.

\section{Introduction}
  Some Introduction.

\section{Theoretical discussion}
In this section, we define and discuss the technique of word embeddings used in many recent natural language processing (NLP) tasks.
We introduce some recent methods for creating word embeddings and compare them.
Our exposition is based on the work of Mikolov et al. in \cite{DBLP:journals/corr/MikolovSCCD13}, \cite{DBLP:journals/corr/abs-1301-3781} and Bengio et al. in \cite{Morin05hierarchicalprobabilistic}.
Further explanations and details can be found in papers by Levy et al. \cite{DBLP:journals/corr/GoldbergL14} and Rong \cite{DBLP:journals/corr/Rong14}.

\subsection{Preliminaries -- stochastic gradient descent}
In this section, we give a brief overview of the technique of \emph{stochastic gradient descent (SGD)}.
Readers familiar with the concept may want to skip this section or come back to it later on.

Stochastic gradient descent is a commonly used family of algorithms for computing
\begin{equation}
  \label{eq:sgd}
  \argmin_\theta \sum_i^n Q_i(\theta),
\end{equation}
the minimizer of a sum of real functions $Q_1, \dots, Q_n$.

Such functions frequently arise in statistics when considering so called \emph{M-Estimators}.
For example, let $(f_\theta)_{\theta \in \Theta}$ be a family of probability densities and $X_1, \dots, X_n$ be independent samples, drawn from the distribution $f_{\theta_0}$ for some $\theta_0 \in \Theta$.
The maximum likelihood estimator $\hat \theta$ for $\theta_0$ is defined by
\begin{align*}
  \hat \theta & = \argmax_{\theta \in \Theta} \prod_{i = 1}^n f_\theta(X_i) \\
              & = \argmin_{\theta \in \Theta} \sum_{i = 1}^n - \log(f_\theta(X_i)),
\end{align*}
so computing $\hat \theta$ amounts to solving the optimization problem (\ref{eq:sgd}) with $Q_i(\theta) = -\log(f_\theta(X_i))$.

Traditional gradient methods try to solve the problem iteratively by producing a sequence of values $\theta_1, \theta_2, \dots$ by the rule
\begin{equation}
  \label{eq:traditional-gradient-descent}
  \theta_{k + 1} = \theta_{k} + \eta \cdot \sum_{i = 1}^n \nabla_\theta Q_i(\theta_i),
\end{equation}
with start value $\theta_1 \in \Theta$ and step size $\eta \in \R$ chosen for the specific problem at hand, that converges under suitable conditions to a local minimum.

Stochastic gradient descent replaces the expensive calculation of $\nabla_\theta Q$ in every step by $\nabla_\theta Q_i$ for just one $i$, so that the update rule (\ref{eq:traditional-gradient-descent}) becomes
\begin{equation}
  \label{eq:stochastic-gradient-descent}
  \theta_{k + 1} = \theta_k + \eta \cdot \nabla_\theta Q_k(\theta_k).
\end{equation}
After calculating $\theta_n$, the process may be repeated by setting $\theta_1 \coloneqq \theta_n$, optionally shuffling the indices $i$ beforehand for better stability.
The sequence $\theta_i$ converges to a local minimum if a suitable smoothness hypothesis holds and the step size $\eta$ is decreased steadily.

\subsection{Word embeddings and distributional hypothesis}

While to a human speaker the relation of the words ``small'' and ``big'' may be obvious, there is no correspondence of meaning and the representation as strings of characters of the two words.
Thus, algorithmically, we can only hope to learn aspects of the meaning of the two words by the contexts they appear in.
The informal statement that this is possible at all, i.e. that the meaning of a word can be reconstructed from contexts in which it is used, is refered to as the ``Distributional Hypothesis'' and was already stated in XY.
Even assuming that this conjecture holds, the task at hand is made very difficult by the discrete nature of text:
In training text data, every context a word appears in may be unique without obvious relation to other appearances.
For example, in the sentences
\begin{equation*}
  \textit{``The needle was small.''}
\end{equation*}
and
\begin{equation*}
  \textit{``A mouse is small.''},
\end{equation*}
the adjective ``small'' is used similarly, while all other words are completely different.
If we hope to learn the meaning of a word from its context, we need an algorithm robust to differences in the actual words used when the meaning of the context is similar.
For example, if we know that the two words ``was'' and ``is'' are actually just conjugated forms of the verb ``to be'' and that ``needle'' and ``mouse'' are nouns, by noticing that ``small'' and other adjectives are used in similar contexts, we can conclude that they have a common property (being adjectives).
It has been shown that word embeddings can provide the needed notion of similarity of words.
A word embedding is a mapping $v : W \rightarrow \R^d$ that assigns a value in $d$-dimensional real space to each word in a finite set of words $W$, with the expectation that vectors of similar words will be close in the embedding space.
Investigation in recent years have shown, that this is a very promising approach for solving many problems in NLP.
Not only can similarity as a whole be encoded by distances of vectors, but also much more subtle relationships of words.
XY et al showed that their word embedding had the property of encoding specific differences in the meaning of words as simple arithmetical differences of their respective embeddings.
For example, they found that there is a constant gender difference vector, so
\begin{equation*}
  v(\text{``king''}) - v(\text{``queen''}) \approx v(\text{``uncle''}) - v(\text{``aunt''}),
\end{equation*}
and that the relationship of countries to their capital cities was encoded similarly:
\begin{equation*}
  v(\text{``Germany''}) - v(\text{``Berlin''}) \approx v(\text{``France''}) - v(\text{``Paris''}).
\end{equation*}

\subsection{Notation}
Throughout the paper, we use the following definitions and notation:
We denote the set of words of the language we consider by $W$.
All algorithms use a sequence of text composed of words from $W$ for training.
We denote this sequence by $T \in W^n$, where $n$ is the length of $T$.
All algorithms we introduce in this paper use a notion of \emph{context} of an occurence of $w \in W$ in $T$.
Fix a \emph{window size} $k \in \N$.
$k$ determines how contexts of words are extracted from $T$ as follows:
Let
\begin{equation*}
  w_{-k}, \dots, w_{-1}, w, w_1, \dots, w_k
\end{equation*}
be any occurence of $w$ in $T$.
We call any (not necessarily contiguous) subsequence of
\begin{equation}
  \label{eq:context}
  (w_{-k}, \dots, w_{-1}, w_1, \dots, w_k)
\end{equation}
a \emph{context} of this occurence of $w$.
Depending on the algorithm, only the full sequence or every single word in (\ref{eq:context}) may be considered a valid context.
In any case, we refer to the set of all valid contexts obtained from $T$ by $C$.

According to the distributional hypothesis, to understand the meaning of a language, it suffices to know its statistical properties.
The algorithms introduced below make use of this statement by estimating parameters based on the distribution of the random variable
\begin{equation}
  (\mathcal{W}, \mathcal{C})
\end{equation}
that is obtained by randomly selecting a word $\mathcal{W} \in W$ from $T$ and its context $\mathcal{C} \in C$ in $T$.
The specifics of the selection process are discussed in section XY.

Our ultimate goal is to create a word embedding $v_W : W \rightarrow \R^d$ for a \emph{dimension} parameter $d \in \N$.
This is usually an arbitrary mapping, which can be represented as a finite dictionary because $W$ is assumed to be finite.

\subsection{Naive continuous-bag-of-words model}
\label{naive-cbow}
The naive \emph{continuous-bag-of-words} (CBOW) algorithm uses the distribution of the $W$-valued random variable $\E(\mathcal{W} \mid \mathcal{C})$ to create word embeddings.
In addition to the word embedding $v_W : W \rightarrow \R^d$, we include second embedding $v_C : W \rightarrow \R^d$ of words, whose values we interpret as \emph{context values} of words.
In the setting of the CBOW model, valid contexts are complete sequences of length $2 k $ as in (\ref{eq:context}) without ellision of further words.
We extend the domain of $v_C$ to the set of such contexts $C$ by the rule
\begin{equation}
  \label{eq:cbow-context-embedding}
  v_C(c) = \frac{1}{|c|} \sum_{w \in c} v_C(w).
\end{equation}
Obviously, (\ref{eq:cbow-context-embedding}) is invariant under permutation of words within the context, thus the name ``bag of words'', implying absence of a specific order (the predicate ``continuous'' was chosen because continuous representations of words, i.e. word embeddings, are created).

We model the categorical data produced by $\E(\mathcal{W} \mid \mathcal{C})$ with a softmax distribution, so that
\begin{equation}
  \label{eq:naive-cbow-model}
  \P(\mathcal{W} = w \mid \mathcal{C} = c) = \exp(v_W(w)^t v_C(c)) \left( \sum_{w' \in W} \exp(v_W(w')^t v_C(c)) \right)^{-1}.
\end{equation}
Our goal is to find the best paramters $v_W, v_C$ that assign the highest probability to training data.
Let $(w_1, c_1), \dots, (w_s, c_s)$ be samples drawn from this distribution.
Assuming independence of these observations, the CBOW estimator $\hat v_{\text{cbow}} = (\hat v_W, \hat v_C)$ is given by
\begin{align}
  \label{eq:cbow-estimator}
  \begin{split}
    \hat v_{\text{cbow}} & = \argmax_{v_W, v_C} \prod_{i = 1}^n \P(\mathcal{W} = w_i \mid \mathcal{C} = c_i) \\
                         & = \argmax_{v_W, v_C} \sum_{i = 1}^n \left( v_W(w_i)^t v_C(c_i) - \log \left( \sum_{w' \in W} v_W(w'_i)^t v_(c) \right) \right),
  \end{split}
\end{align}
which is a problem of type (\ref{eq:sgd}) and could theoretically be computed via SGD.
Unfortunately, the gradient of the normalization term
\begin{equation}
  \label{eq:cbow-normalization}
  \log \left( \sum_{w' \in W} v_W(w'_i)^t v_C(c) \right)
\end{equation}
has to be evaluted in every update step of SGD, which makes every iteration in SGD a $\mathcal{O}(|W|)$ operation.
In practice, this cost is to high, so the estimator $\hat v_{\text{cbow}}$ has only theoretical relevance.

\subsection{Naive skip-gram model}
\label{naive-skip-gram}
The \emph{skip-gram} model may be viewed as dual to the CBOW model introduced in \ref{naive-cbow}.
Instead of predicting a word from a given context, skip-gram predicts a context from the word, i.e. $\E(\mathcal{C} \mid \mathcal{W})$.
Like the cbow model, the skip-gram model includes a second word embedding $v_C : W \rightarrow \R^d$ which may be interpreted as the words' context values.
Unlike the cbow model, however, in the skip-gram model, each context consists of only a single word so that each word occurence has up to $2 k$ contexts.
$k$ is now the maximal distance between two words for them to be still considered in each other's contexts.
Analogously to (\ref{eq:naive-cbow-model}), we define
\begin{equation}
  \label{eq:naive-skip-gram-model}
  \P(\mathcal{C} = c \mid \mathcal{W} = w) = \exp(v_W(w)^t v_C(c)) \left( \sum_{c' \in C} \exp(v_W(w)^t v_C(c')) \right)^{-1}
\end{equation}
so that the maximum likelihood estimator $\hat v_{\text{skip-gram}} = (\hat v_W, \hat v_C)$ for independent data $(w_1, c_1), \dots, (w_s, c_s)$ is given by
\begin{equation}
  \hat v_{\text{skip-gram}} = \argmax_{v_W, v_C} \sum_{i = 1}^n \left( v_W(w_i)^t v_C(c_i) - \log \left( \sum_{c' \in C} v_W(w_i)^t v_C(c') \right) \right).
\end{equation}
As is the case for the case for the cbow estimator (\ref{eq:cbow-estimator}), using $\hat v_{\text{skip-gram}}$ is not viable in practice because computing the gradient of
\begin{equation}
  \label{eq:skip-gram-normalization}
  \log \left( \sum_{c' \in C} v_W(w_i)^t v_C(c') \right)
\end{equation}
is too costly.

\subsection{Hierarchical softmax}
In \ref{naive-skip-gram}, we identified the evaluation of the normalization term (\ref{eq:skip-gram-normalization}) as bottleneck for the computation of $\hat v_{\text{skip-gram}}$.
Hierarchical softmax replaces the usual ``flat'' softmax (\ref{eq:naive-skip-gram-model}) by a hierarchical softmax distribution, thereby making the computation of the probability of a single outcome much less costly.
The idea is create a binary tree whose leaves are the outcomes of the experiment.
Each internal node assigns a probability the their left and right nodes.
The probability of an outcome is determined by a random walk starting at the root node that ends at the specific outcome.

More formally, let $D$ be a binary tree whose leaves are exactly the words $W$.
Replace the model parameter $v_C$ by an embedding $v_N : \Int(D) \rightarrow \R^d$, where $\Int(D)$ denotes the set of all internal (i.e.\@ non-leaf) nodes of $D$.
Let $s :\Int(D) \setminus \{\rootnode(D)\} \rightarrow \{1, -1\}$ with 
\begin{equation*}
  s(N) = \begin{cases}
    1 & \text{if N is a left child} \\
    -1 & \text{otherwise.}
      \end{cases}
\end{equation*}
($s$ may be chosen arbitrarily, we only need that $s(N_1) = -s(N_2)$ for siblings $N_1, N_2$.) \\
For every $w \in W$, there is a unique path 
\begin{equation*}
  \rootnode(D) = l(w, 0), l(w, 1), \dots, l(w, n_w) = w
\end{equation*}
from the root node to $w$.
As was the case in the skip-gram model, we only consider contexts consisting of a single word.
Define
\begin{equation}
  \P_v(\mathcal{C} = c \mid \mathcal{W} = w) = \prod_{i = 0}^{n_c - 1} \sigma \left( s(l(c, i + 1)) \cdot v_W(w)^t v_N(l(c, i)) \right)
\end{equation}
where $\sigma$ is the sigmoid function
\begin{equation*}
  \sigma(x) = \frac{1}{1 + \exp(-x)},
\end{equation*}
the special case of the argmax density for two classes.

Clearly $\P(\mathcal{W} = w \mid \mathcal{C} = c)$ only depends on the embeddings of ancestor nodes of $c$ and on the embedding of $w$.
If $D$ is balanced and the context size is bounded, these are only $\mathcal{O}(\log(|W|)$ values, which makes calculating the maximum likelihood estimator via SGD computationally viable.

The method can be adapted to the CBOW model.
However, while $v_N$ replaces $v_C$ in the skip-gram, to adapt hierarchical softmax to cbow, $v_W$ has to be replaced by $v_N$.
This means that the final word embeddings produced have to be retrieved from $v_C$.

Word embeddings learned by hierarchical softmax based techniques depend on the choice of $D$.
Mikolov et al.\@ construct a Huffman tree from $W$ to speed up computation of $\P_v(\cdot \mid c)$ for frequently used words by placing these words near the root node, while Bengio et al.\@ derive the tree structure from the WordNet \cite{WordNet} semantic database so that semantically related words are placed near to each other in the tree.

\subsection{Negative Sampling}
Consider the following problem:
Suppose given a pair $(w, c)$ of word and context.
Was $(w, c)$ extracted from actual text or was it sampled from a noise distrubtion?
We model this decision as a stochastic mapping $\mathcal{D} : W \times C \rightarrow \{0, 1\}$ with $\mathcal{D}(w, c) = 1$ if and only if $(w, c)$ stems from text.
In the process of learning the function $\mathcal{D}$, we hope to create good word embeddings.
As in \ref{naive-cbow} and \ref{naive-skip-gram}, we use $v = (v_W, v_C)$ as parameter in the logistic model
\begin{equation}
  \P_v(D(w, c) = 1) = \sigma(v(w)^t v(c)) = \frac{1}{1 - \exp(-v(w)^t v(c))}.
\end{equation}
Let $(w_1, c_1), \dots, (w_n, c_n)$ be word -- context pairs extracted from text.
For each $i \in \{1, \dots, n\}$, sample $s$ pairs $(w_{i, 1}, c_{i, 1}, \dots, w_{i, s}, c_{i, s})$ of negative examples from a noise distribution $Q$, where $s \in \N$ is a fixed value, the \emph{negative sampling rate}.
The maximum likelihood estimator $\check v$ for $v$ -- assuming independence of all observations -- is given by
\begin{align}
  \label{eq:ns-estimator}
  \begin{split}
    \check v & = \argmin_v \prod_{i=1}^n \left( \sigma(v_W(w_1)^t v_C(c_1)) \prod_{j = 1}^s \sigma(v_W(w_{i, j})^t v_C(c_{i, j})) \right) \\
             & = \argmin_v \sum_{i=1}^n \left( \log \left(\sigma(v_W(w_1)^t v_C(c_1)) \right) - \sum_{j = 1}^s \log \left( \sigma(v_W(w_{i, j})^t v_C(c_{i, j})) \right) \right),
  \end{split}
\end{align}
which can be calculated by SGD.

In the cbow model, only $\E(W \mid \mathcal{C})$, a variable of words, is truly random, so it makes sense to only generate words randomly.
Thus, for a given $(c_i, w_i)$ from sample data, the negative examples are
\begin{equation*}
  (w_{i, 1}, c_i, \dots, w_{i, s}, c_i),
\end{equation*}
for randomly sampled words $w_{i, j}$.
Analogously, in skip-gram negative sampling,
\begin{equation*}
  (w_i, c_{i, 1}, \dots, w_i, c_{i, s}),
\end{equation*}
with random contexts $c_{i, j}$.

Note that in both cases, it suffices to sample random words, because contexts consist of only a single word in the skip-gram model.
In empirical tests, Mikolov et al.\@ found the noise distribution
\begin{equation}
  \label{eq:noise-distribution}
  Q(\{w\}) = \frac{{f_w}^{3 \over 4}}{\sum_{w' \in W} {f_{w'}}^{3 \over 4}}
\end{equation}
on words, with $f_w$ the word frequency of $w \in W$ in training text, to yield significantly better results than either the uniform distribution or the unscaled frequency distribution.
To our knowledge, there is no theoretical explanation for this result.
$Q$ is a slightly smoothed frequency distribution, so it seems a small bias towards less frequently used words works well with NEG.

Furthermore, Mikolov et al.\@ found that the negative sampling rate $s$ can be fairly small.
On small data sets, they $5 \leq s \leq 20$ was determined to be a good choice, while on larger data sets, even $2 \leq s \leq 5$ was sufficient.

The biggest advantage of negative sampling compared to the other techniques is its simplicity.
Because the update step for SGD with objective (\ref{eq:ns-estimator}) is an $\mathcal{O}(1)$ operation, training time grows only linearly with $|W|$ so that training on large data sets is feasible.
Mikolov et al.\@ showed that this advantage was sufficient to outperform more sophisticated techniques such as hierarchical softmax or deep neural networks (in fact, (\ref{eq:ns-estimator}) can be interpreted as optimizing a shallow neural network) in various NLP tasks such as analogy tests.

\subsection{Data preprocessing and subsampling}
Some words of natural languages are used much more frequently tha others, for example ``a'' or ``the''.
Because of this, they do not carry less information than rare words.
On the other hand, a higher share of overall training time is spent on the vectors of frequent words, because there are typically more samples in which they occure.
To counter this imbalance, frequently words can be \emph{subsampled} before training.

Mikolov et al.\@ preprocess text data $T$ before training as follows:
First, all the frequency of every word relative to the whole text corpus is calculated (this has to be done when training via negative sampling anyway because the noise distribution $Q$ in (\ref{eq:noise-distribution}) depends on these values).
Denote the frequency of $w$ by $f_w$.
Then, every occurence of a word $w$ is discarded from $T$ with probability
\begin{equation}
  \label{eq:downsampling-probability}
  \max \left\{0, 1 - \sqrt{\frac{t}{f_w}} \right\}
\end{equation}
for a \emph{downsampling threshold} $t \in [0, 1]$.
Notice that (\ref{eq:downsampling-probability}) is 0 whenever $f_w \leq t$, so only words $w$ with $f_w > t$ are downsampled.
The formula (\ref{eq:downsampling-probability}) was found by experiment.
Statistically, it preserves the ranking of frequencies (i.e. frequent words are more frequent after preprocessing), while subsampling frequent words aggressively.
Mikolov et al.\@ found that a downsampling threshold between $0$ and $10^{-5}$ worked well in their experiments.

Subsampling can increase the quality of word embeddings while at the same time reduce training time dramatically for all algorithms we discussed.
However, while semantic tasks benefit from subsampling, performance on syntactic tasks degrades slightly.
As the subsampling process we described above can yield grammatically incorrect text very often, this was to be expected.

From an optionally subsampled text, Mikolov et al. extract all possible pairs of word and context that occur whithin to be used in training.
As all estimators we discussed assume stochastically independent data, this may seem questionable.
For example, it seems plausible that word occurences at adjacent positions in text is highly correlated.
Indeed, independence of such events would refute the distributional hypothesis.

However, for sufficiently large amounts of training text, independence of these events is a good approximation.
For big $|T|$, it is reasonable to expect the distribution of the randomly permuted sequence $(w_{\sigma(1)}, c_{\sigma(1)}), \dots, (w_{\sigma(n)}, c_{\sigma(n)})$ to approximate a sequence of randomly drawn samples from the language at hand (consider the case where $T$ \emph{is} all valid text of the language).
Because all estimators we discussed are invariant under reordering of samples, we may omit the shuffling process, although it makes SGD more stable in practice.

\subsection{Distance of word vectors}
The word embeddings produced by the algorithms above do not result in very good word embeddings if similarity in $\R^d$ is measured by the usual euclidean norm $\lVert \cdot \rVert_2$.
However, for a different notion of distance, the quality of these word embeddings improves significantly.
Measure the distance $q(x, y)$ of vectors $x, y \in \R^d$ by their \emph{cosine distance}, i.e.
\begin{equation*}
  q(x, y) = 1 - \frac{x^t y}{\lVert x \rVert_2 \lVert y \rVert_2} = 1 - \cos(\alpha)
\end{equation*}
where $\alpha$ is the angle between $x$ and $y$.
Thus, vectors are considered close if their direction is similar (so $\cos(\alpha)$ is close to 1) and far away if they face opposite directions (so $\cos(\alpha)$ is nearly -1).
In the setting of both skip-gram or CBOW, the sample probability will be large if we maximize
\begin{equation*}
  v_W(w)^t v_C(c)
\end{equation*}
for every word $w \in W$ that occurs in context $c \in C$, thereby reducing the pairwise distance of such words, and simultaneously maximize the angle between vectors of words that do not occure in similar contexts.

\subsection{Comparison}
Mikolov et al. report that the estimators based on negative sampling outperformed the ones based on hierarchical softmax \cite{DBLP:journals/corr/abs-1301-3781}.
Furthermore, all other parameters fixed, the the skip-gram estimator yielded better results than the CBOW estimator for large corpora.
They found that word embeddings produced by cbow were overly smooth, which was be to be expected, given that cbow embeds contexts by average of individual words (\ref{eq:cbow-context-embedding}).
On the other hand, the skip-gram model takes very word in the context into account separately, which makes the algorithms less prone to oversmoothing.
Bengio et al. \cite{Morin05hierarchicalprobabilistic} counter this by including an additional matrix $M \in \R^{d \times 2dk}$ as parameter in the estimation process that is used as projection matrix to embed contexts $c = (w_1, \dots, w_{2k})$ into $\R^d$ via
\begin{equation}
  v_C(c) = M (v_C(w_1)^t, \dots, v_C(w_{2k})^t)^t.
\end{equation}
Unfortunately, it seems there is no direct comparison of the two approaches in literature.

All experiments on word embeddings we mentioned have been conducted for the English language.
To the best of our knowledge, there are no published results on word embeddings for German.
There are some problems that arise when adapting the algorithms we introduced to this new language.
In all the explanations up until now, we assumed a finite vocabulary set $W$, which is true for the English language.
However, there are infinitely many German words because nouns can be compounded to form new words.
We found that this was not a big problem for our task.
Most compound words are either never used or frequent enough to create sufficiently good word vectors.

\printbibliography

\end{document}

